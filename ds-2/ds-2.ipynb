{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas_datareader as pdr\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I want to do is to make a dictionary of the datasets I'm interested in. I chose to analyze real estate data in the commercial and residential realm, as well as unemployment rate and cpi less food and energy. The reasoning here is according to Wilshire's fact sheet, they index companies that operate in both commercial and residential real estate, so I included those two; unemployment rate plus the cpi less F&E could help add general economic conditions to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_dict = {\n",
    "    're_comm': 'CREACBW027SBOG', # commercial real estate loans, weekly\n",
    "    're_resd': 'RHEACBW027SBOG', # residential real estate loans, weekly\n",
    "    'un_rate': 'UNRATE', # unemployment, monthly\n",
    "    'cpi_less_fe': 'CORESTICKM159SFRBATL', # sticky price CPI, not counting food and energy, monthly\n",
    "    're_wilshire': 'WILLREITIND' # the wilshire real estate market index, daily\n",
    "}\n",
    "\n",
    "# I chose to set the start dates for some of these series to earlier than exactly 5 years ago to hopefully \n",
    "# capture a little more data. Better to have to discard than extrapolate.\n",
    "\n",
    "datasets = {\n",
    "    're_comm': pdr.get_data_fred(\"CREACBW027SBOG\",start='2017-10-14'), # commercial real estate loans, weekly\n",
    "    're_resd': pdr.get_data_fred(\"RHEACBW027SBOG\",start='2017-10-14'), # residential real estate loans, weekly\n",
    "    'un_rate': pdr.get_data_fred(\"UNRATE\",start='2017-10-01'), # unemployment, monthly\n",
    "    'cpi_less_fe': pdr.get_data_fred(\"CORESTICKM159SFRBATL\",start='2017-10-01'), # sticky price CPI, not counting food and energy, monthly\n",
    "    're_wilshire': pdr.get_data_fred(\"WILLREITIND\",start='2017-10-26') # the wilshire real estate market index, daily\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'll go ahead and resample these data. I'll fill in the `NAN`s by carrying the data points forward until a new data point is present. It's a little wacky but I didn't want to try to extrapolate and fill in too much missing data, so holding the less frequently updated sets constant seemed like a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         datasets_daily[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_daily\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m datasets[dataset]\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mffill()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrename_axis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeSeries\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(datasets[dataset])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets_daily:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3804\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3806\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "datasets['re_wilshire'] = datasets['re_wilshire'].ffill()\n",
    "datasets_daily = {'re_wilshire_daily': datasets['re_wilshire']}\n",
    "for dataset in datasets:\n",
    "    if dataset != 're_wilshire':\n",
    "        datasets_daily[f\"{dataset}_daily\"] = datasets[dataset].resample('D').ffill()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up my sql tables here. I think sqlalchemy is working on a more automated way to map tables into SQL but here I'll just brute force it and create tables for both the original data sets and my resampled dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DECIMAL, DateTime\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class re_comm(Base):\n",
    "    __tablename__ = 're_comm'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('CREACBW027SBOG', Float)\n",
    "\n",
    "class re_resd(Base):\n",
    "    __tablename__ = 're_resd'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('RHEACBW027SBOG', Float)\n",
    "\n",
    "class un_rate(Base):\n",
    "    __tablename__ = 'un_rate'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('UNRATE', Float)\n",
    "\n",
    "class cpi_less_fe(Base):\n",
    "    __tablename__ = 'cpi_less_fe'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('CORESTICKM159SFRBATL', Float)\n",
    "\n",
    "class re_wilshire(Base):\n",
    "    __tablename__ = 're_wilshire'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('WILLREITIND', DECIMAL)\n",
    "\n",
    "class re_comm_daily(Base):\n",
    "    __tablename__ = 're_comm_daily'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('CREACBW027SBOG_RS', Float)\n",
    "\n",
    "class re_resd_daily(Base):\n",
    "    __tablename__ = 're_resd_daily'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('RHEACBW027SBOG_RS', Float)\n",
    "\n",
    "class un_rate_daily(Base):\n",
    "    __tablename__ = 'un_rate_daily'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('UNRATE_RS', Float)\n",
    "\n",
    "class cpi_less_fe_daily(Base):\n",
    "    __tablename__ = 'cpi_less_fe_daily'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    date = Column('TimeSeries', DateTime, index=True)\n",
    "    data = Column('CORESTICKM159SFRBATL_RS', Float)\n",
    "\n",
    "with open('../.pgpass', 'r') as f:\n",
    "    host, port, database, user, password = f.read().split(':')\n",
    "\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}', echo=False)\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will commit the original and resampled dataframes to SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['re_comm'].to_sql(name=re_comm.__tablename__, con=engine, if_exists='replace')\n",
    "datasets['re_resd'].to_sql(name=re_resd.__tablename__, con=engine, if_exists='replace')\n",
    "datasets['un_rate'].to_sql(name=un_rate.__tablename__, con=engine, if_exists='replace')\n",
    "datasets['cpi_less_fe'].to_sql(name=cpi_less_fe.__tablename__, con=engine, if_exists='replace')\n",
    "datasets['re_wilshire'].to_sql(name=re_wilshire.__tablename__, con=engine, if_exists='replace')\n",
    "datasets_daily['re_comm_daily'].to_sql(name=re_comm_daily.__tablename__, con=engine, if_exists='replace')\n",
    "datasets_daily['re_resd_daily'].to_sql(name=re_resd_daily.__tablename__, con=engine, if_exists='replace')\n",
    "datasets_daily['un_rate_daily'].to_sql(name=un_rate_daily.__tablename__, con=engine, if_exists='replace')\n",
    "datasets_daily['cpi_less_fe_daily'].to_sql(name=cpi_less_fe_daily.__tablename__, con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
